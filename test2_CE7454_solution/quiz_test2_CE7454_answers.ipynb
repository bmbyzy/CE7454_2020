{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE7454 : Deep Learning for Data Science\n",
    "##Â Xavier Bresson\n",
    "\n",
    "\n",
    "## Quiz\n",
    "Date: October 21st, 2020<br>\n",
    "\n",
    "*Instructions* <br>\n",
    "Name: Do not forget to add your name to the notebook file \"quiz_test2_CE7454_YOUR_NAME.ipynb\".<br>\n",
    "Questions: This notebook has 10 questions.<br>\n",
    "Answers: Write the answers to each question in this notebook.<br>\n",
    "Recommendation: Write concise and to-the-point answers.<br>\n",
    "Type: This test is individual and open-book.<br>\n",
    "Grading: 1 point for each question.<br>\n",
    "LaTeX: If you want to write mathematical equations, you are free to use LaTeX or not. <br>\n",
    "Output/Timestamp: There is no point if the cell has no output or the timestamp is beyond **7:30pm**.<br>\n",
    "Delivery: Upload your notebook to https://drive.google.com/drive/folders/1XCjHocibKqoTpoOc6qlhQBJNwsFAXzJb  by **7:35pm**. <br>\n",
    "Remark: **If certain conditions of the questions (for eg. choice of activation function) are not stated, you are free to choose anything you want.**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "What is a tensor? Define a scalar and a matrix in terms of tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "Tensors are generic multi-dimensional arrays, that can have any dimension. \n",
       "\n",
       "Matrices are 2-dimensional tensors and scalars are 0-dimensional tensors.\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Tensors are generic multi-dimensional arrays, that can have any dimension. \n",
    "\n",
    "Matrices are 2-dimensional tensors and scalars are 0-dimensional tensors.\n",
    "    \n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TT3HcmXNhNxp"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Training neural networks can be done either in serial mode, mini-batch mode, or full batch mode.\n",
    "\n",
    "State which one of these three modes is the best practical choice for training neural networks and write one advantage  for the selected choice with respect to the two other modes.\n",
    "\n",
    "Next, consider the two following plots :\n",
    "\n",
    "<img width=700 src=\"pic_ce7454_01.png?arg\">\n",
    "\n",
    "Which plot corresponds to full batch training and mini-batch training? Justify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "The best practical choice is mini-batch training.\n",
       "\n",
       "Advantage over serial : Mini-batch is faster than serial on GPU.\n",
       "\n",
       "Advantage over full-batch: Fast weight updates and better generalization performances.\n",
       "\n",
       "The left plot corresponds to full batch training and the right plot to mini-batch training. \n",
       "Mini-batch training uses approximate gradients, which explains the oscillations.\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "The best practical choice is mini-batch training.\n",
    "\n",
    "Advantage over serial : Mini-batch is faster than serial on GPU.\n",
    "\n",
    "Advantage over full-batch: Fast weight updates and better generalization performances.\n",
    "\n",
    "The left plot corresponds to full batch training and the right plot to mini-batch training. \n",
    "Mini-batch training uses approximate gradients, which explains the oscillations.\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What is overfitting?\n",
    "\n",
    "How do you reduce overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "Overfitting happens when the loss/error value is zero on the training data.\n",
       "\n",
       "Overfitting is reduced when minimizing the number of parameters to learn.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Overfitting happens when the loss/error value is zero on the training data.\n",
    "\n",
    "Overfitting is reduced when minimizing the number of parameters to learn.\n",
    "\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Selection of the learning rate for the gradient descent algorithm is critical for training. What is the common strategy to select the learning rate?\n",
    "\n",
    "Next, consider the following plot:\n",
    "\n",
    "<img width=400 src=\"pic_ce7454_02.png?arg\">\n",
    "\n",
    "What could explain the two steep drops?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "Common strategy is to start with large learning rate and reduce its value when moving closer to the minimum. \n",
       "\n",
       "The two steep drops are due to a change of learning rate value after the loss value stays constant for some time.\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Common strategy is to start with large learning rate and reduce its value when moving closer to the minimum. \n",
    "\n",
    "The two steep drops are due to a change of learning rate value after the loss value stays constant for some time.\n",
    "    \n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Consider the following neural network that solves the two-class classification problem :\n",
    "\n",
    "$$y = \\sigma (W x)$$\n",
    "\n",
    "with $x$ the input data of size $(d,1)$ where $d$ is the number of features. \n",
    "\n",
    "If the non-linear activation function $\\sigma$ is the sigmoid function, what are the sizes of output $y$ and matrix $W$? \n",
    "\n",
    "What are the sizes of output $y$ and matrix $W$ when the non-linear activation function $\\sigma$ is the softmax function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "If $\\sigma$ is the sigmoid function, then $y$ is of dimension 1 and $W$ of size is (1,d).\n",
       "\n",
       "If $\\sigma$ is the softmax function, then $y$ is of dimension 2 and $W$ of size is (2,d).\n",
       "\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "If $\\sigma$ is the sigmoid function, then $y$ is of dimension 1 and $W$ of size is (1,d).\n",
    "\n",
    "If $\\sigma$ is the softmax function, then $y$ is of dimension 2 and $W$ of size is (2,d).\n",
    "\n",
    "    \n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Consider the task of predicting a sentiment class (for example, positive or negative sentiment) for fixed 10-word-length sentences. \n",
    "\n",
    "Consider two neural network models, a multi-layer perceptron (MLP) and a recurrent neural network (RNN) trained on a large dataset of 10-word sentences. Which network is a better choice? Justify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "RNN because its architecture is designed to capture sequential information (order of words).\n",
       "\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "RNN because its architecture is designed to capture sequential information (order of words).\n",
    "\n",
    "    \n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Describe the problems of vanishing and exploding gradient?\n",
    "\n",
    "How to solve/mitigate these problems in the case of convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "Vanishing gradient problem happens when the gradient of the loss function approaches or is equal to zero, \n",
       "stopping the training of the network. \n",
       "\n",
       "Activation functions, like sigmoid or tanh, have derivative close or equal to zero for large positive \n",
       "or negative values. As such, they suffer from vanishing gradient.\n",
       "\n",
       "Exploding gradient problem happens when the gradient of the loss function gets very large, \n",
       "making the value of the network weights diverging. \n",
       "\n",
       "For CNNs, the vanishing problem is solved with ReLU activation. This network has no issue with exploding gradient.\n",
       "\n",
       "For RNNs, the vanishing problem is diminished with a long-term memory state as introduced in LSTM. \n",
       "The exploding gradient is controled by either clipping the gradient value or normalizing the gradient.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Vanishing gradient problem happens when the gradient of the loss function approaches or is equal to zero, \n",
    "stopping the training of the network. \n",
    "\n",
    "Activation functions, like sigmoid or tanh, have derivative close or equal to zero for large positive \n",
    "or negative values. As such, they suffer from vanishing gradient.\n",
    "\n",
    "Exploding gradient problem happens when the gradient of the loss function gets very large, \n",
    "making the value of the network weights diverging. \n",
    "\n",
    "For CNNs, the vanishing problem is solved with ReLU activation. This network has no issue with exploding gradient.\n",
    "\n",
    "For RNNs, the vanishing problem is diminished with a long-term memory state as introduced in LSTM. \n",
    "The exploding gradient is controled by either clipping the gradient value or normalizing the gradient.\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What is the purpose of the memory state vector in a recurrent neural network (RNN)?\n",
    "\n",
    "How is the memory state vector used in the translation task?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "The memory state vector is designed to maintain information in memory for a long period of time.\n",
       "It provides a summary of the sequence.\n",
       "\n",
       "For translation, the memory state vector is used to first summarize the sentence in source language \n",
       "and then decode the sentence in the target language.\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "The memory state vector is designed to maintain information in memory for a long period of time.\n",
    "It provides a summary of the sequence.\n",
    "\n",
    "For translation, the memory state vector is used to first summarize the sentence in source language \n",
    "and then decode the sentence in the target language.\n",
    "    \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Compute the value of the output gradients $\\frac{\\partial L}{\\partial X_{11}},\\frac{\\partial L}{\\partial X_{12}},\\frac{\\partial L}{\\partial X_{21}},\\frac{\\partial L}{\\partial X_{22}}$ for the average pooling layer :\n",
    "\n",
    "<img width=600 src=\"pic_ce7454_03a.png?arg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "$$\\frac{\\partial L}{\\partial X_{ij}} = \\frac{-4.1}{4} \\ \\forall ij$$\n",
       "\n",
       "<img width=600 src=\"pic_ce7454_03b.png?arg\">\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "$$\\\\frac{\\partial L}{\\partial X_{ij}} = \\\\frac{-4.1}{4} \\\\ \\\\forall ij$$\n",
    "\n",
    "<img width=600 src=\"pic_ce7454_03b.png?arg\">\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "How to implement a linear layer with a convolutional layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 20-10-21--10-17-16\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    \n",
       "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
       "\n",
       "**YOUR ANSWER HERE**\n",
       "\n",
       "A linear layer can be implemented with a convolutional layer with kernel size 1.\n",
       "\n",
       "Example: Let's consider an input image $x$ of size $(n,n,d)$, with $n$ pixels and $d$ color features. \n",
       "\n",
       "Linear layer: Let's reshape $x$ to $(n^2,d)$ and multiplies it with a matrix of size $(d,e)$. \n",
       "It produces a matrix of size $(n^2,e)$ that can be reshaped to size $(n,n,e)$.\n",
       "\n",
       "Convolutional layer: The same output can be produced with a convolutional filter of size $(e,d,1,1)$,  \n",
       "with $d$ color features and $e$ activation maps and pixel size $(1,1)$. \n",
       "The input of the convolutional layer is of size $(n,n,d)$ and the output of size $(n,n,e)$.\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(\"\"\"\n",
    "    \n",
    "**DO NOT FORGET TO RUN THIS CELL TO GET A TIMESTAMP**\n",
    "\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "A linear layer can be implemented with a convolutional layer with kernel size 1.\n",
    "\n",
    "Example: Let's consider an input image $x$ of size $(n,n,d)$, with $n$ pixels and $d$ color features. \n",
    "\n",
    "Linear layer: Let's reshape $x$ to $(n^2,d)$ and multiplies it with a matrix of size $(d,e)$. \n",
    "It produces a matrix of size $(n^2,e)$ that can be reshaped to size $(n,n,e)$.\n",
    "\n",
    "Convolutional layer: The same output can be produced with a convolutional filter of size $(e,d,1,1)$,  \n",
    "with $d$ color features and $e$ activation maps and pixel size $(1,1)$. \n",
    "The input of the convolutional layer is of size $(n,n,d)$ and the output of size $(n,n,e)$.\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNd30TjgREYWyAkV0lT4iX6",
   "name": "CE_7454_coding_test_solution_Vijay.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
